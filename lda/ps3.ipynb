{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncollapsed Gibbs Sampling for Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Below, we use text from posts on [beer.stackoverflow.com](https://archive.org/details/stackexchange) and an uncollapsed Gibbs Sampling algorithm for latent topic analysis.\n",
    "\n",
    "We also use the perplexity measure to evaluate different values for K - the number of topics. As expected (and intuitively sensical), this measure suggests more topics fit the data better.\n",
    "\n",
    "The results of LDA are presented as the most likely words for each topic. It is a matter of debate whether these topics are interpretable or useful, but hey, at least it's about beer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to parse text: 0.113223075867\n",
      "['I was offered a beer the other day that was reportedly made with citra hops. What are citra hops? Why should I care that my beer is made with them?', 'As far as we know, when did humans first brew beer, and where? Around when would you have been able to get your hands on something resembling a modern lager?']\n",
      "Total number of documents: 1000\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import cElementTree as ET\n",
    "import sys\n",
    "from HTMLParser import HTMLParser\n",
    "import time\n",
    "\n",
    "# credit: http://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "posts = open('Posts.xml', 'r').read()\n",
    "\n",
    "posts[1:100]\n",
    "\n",
    "def remove_tags(text):\n",
    "    return ''.join(ET.fromstring(text).itertext())\n",
    "\n",
    "root = ET.fromstring(posts)\n",
    "documents = []\n",
    "t0 = time.time()\n",
    "for child in root.findall('row')[0:1000]:\n",
    "    text = None\n",
    "    child_text = child.get('Body').encode('utf-8').strip()\n",
    "    text = strip_tags(child_text)\n",
    "    documents.append(text)\n",
    "t1 = time.time()    \n",
    "\n",
    "print 'Time to parse text: ' + str(t1 - t0)\n",
    "print documents[0:2]\n",
    "print 'Total number of documents: ' + str(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting terms for doc: 0\n",
      "counting terms for doc: 25\n",
      "counting terms for doc: 50\n",
      "counting terms for doc: 75\n",
      "counting terms for doc: 100\n",
      "counting terms for doc: 125\n",
      "counting terms for doc: 150\n",
      "counting terms for doc: 175\n",
      "counting terms for doc: 200\n",
      "counting terms for doc: 225\n",
      "counting terms for doc: 250\n",
      "counting terms for doc: 275\n",
      "counting terms for doc: 300\n",
      "counting terms for doc: 325\n",
      "counting terms for doc: 350\n",
      "counting terms for doc: 375\n",
      "counting terms for doc: 400\n",
      "counting terms for doc: 425\n",
      "counting terms for doc: 450\n",
      "counting terms for doc: 475\n",
      "counting terms for doc: 500\n",
      "counting terms for doc: 525\n",
      "counting terms for doc: 550\n",
      "counting terms for doc: 575\n",
      "counting terms for doc: 600\n",
      "counting terms for doc: 625\n",
      "counting terms for doc: 650\n",
      "counting terms for doc: 675\n",
      "counting terms for doc: 700\n",
      "counting terms for doc: 725\n",
      "counting terms for doc: 750\n",
      "counting terms for doc: 775\n",
      "counting terms for doc: 800\n",
      "counting terms for doc: 825\n",
      "counting terms for doc: 850\n",
      "counting terms for doc: 875\n",
      "counting terms for doc: 900\n",
      "counting terms for doc: 925\n",
      "counting terms for doc: 950\n",
      "counting terms for doc: 975\n",
      "Sanity checks:\n",
      "\tTime to build corpus: 3.10362291336\n",
      "\tTerms in first document: [u'wa' u'offer' u'beer' u'day' u'wa' u'reportedli' u'citra' u'hop' u'citra'\n",
      " u'hop' u'whi' u'care' u'beer']\n",
      "\tTotal words in corpus: 58205\n",
      "\tNumber docs in corpus: 1000\n",
      "\tNumber of unique words in corpus: 5879\n"
     ]
    }
   ],
   "source": [
    "execfile('corpus.py')\n",
    "execfile('document.py')\n",
    "\n",
    "t0 = time.time()\n",
    "corpus = Corpus(documents, '../Week1HW/stopwords.txt', 2)\n",
    "t1 = time.time()\n",
    "\n",
    "corpus.generate_document_term_matrix()\n",
    "termlist = list(corpus.token_set)\n",
    "print 'Sanity checks:'\n",
    "print '\\tTime to build corpus: ' + str(t1 - t0)\n",
    "print '\\tTerms in first document: ' + str(corpus.docs[0].tokens)\n",
    "print '\\tTotal words in corpus: ' + str(corpus.ntotal_tokens)\n",
    "print '\\tNumber docs in corpus: ' + str(corpus.N)\n",
    "print '\\tNumber of unique words in corpus: ' + str(len(corpus.token_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time metrics\n",
    "\n",
    "Below are for K = 3\n",
    "\n",
    "* For 1694 documents with 7942 unique tokens and 118617 words, the time to run the gibbs loop for 10 iterations was 7.4 minutes.\n",
    "* For 200 documents 10 gibbs loop iterations took 13.88 seconds\n",
    "* For 200 documents, 100 gibbs loop iterations took 143 seconds\n",
    "\n",
    "So it makes sense that running gibbs for all documents will take 70 minutes with K = 3\n",
    "\n",
    "#### Note: Computing time doesn't grow with K, only D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "    1. Randomly assign all words to a topic\n",
    "    2. Initialize theta (D x K, document-specific topic probabilities): each theta_d is a k-dimensional sample from Dir(alpha + number of words in topic k)\n",
    "    3. Initialize beta (K x V, topic-specific term probabilities): each beta_k is a V-dimensional sample from Dir(eta + number of times term v appeared in topic k)\n",
    "\n",
    "#### Repeat\n",
    "\n",
    "    1. for every word wi in N, it's current allocation is zi\n",
    "        1.1. decrement -1 zi document-topic count and topic-term count\n",
    "        1.2. Use theta and beta to calculate probability over topic for word (slide 14 lecture6)\n",
    "        1.3. draw from multinomial with these probabilites - this is the new allocation zi for word wi\n",
    "        1.4. increment +1 zi document-topic count and topic-term count\n",
    "    2. update theta and beta\n",
    "\n",
    "#### See `corpus.py` for this code: in `lda_gibbs()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of K\n",
    "\n",
    "To select the number of topics, one may use the perplexity measure. This measure is interpretable as the number of occurences of term v times it's total probability as estimated by theta and beta probability vectors, normalized by the total number of terms in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(corpus, probs, K):\n",
    "    termlist = list(corpus.token_set)\n",
    "    numerator_sum = 0\n",
    "    for doci, doc in enumerate(corpus.docs):\n",
    "        doc_prob_sum = 0\n",
    "        for wordi, word in enumerate(doc.tokens):\n",
    "            termidx = termlist.index(word)\n",
    "            k_sum = 0\n",
    "            # add to doc sum 1*prob of term for each k\n",
    "            for k in range(K):\n",
    "                k_sum += probs['theta'].item((doci, k))*probs['beta'].item((k, termidx))\n",
    "            doc_prob_sum += np.log(k_sum)\n",
    "        numerator_sum += doc_prob_sum\n",
    "    return np.exp(-(numerator_sum/corpus.ntotal_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realize the right thing to do here is parallelize, but it's not straight-forward to parrallelize gibbs sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most likely terms for topic 0:\n",
      "drink with probability 0.0523802103245\n",
      "can with probability 0.036042161211\n",
      "beer with probability 0.0359501071678\n",
      "realli with probability 0.0255232138177\n",
      "becaus with probability 0.0232588872648\n",
      "ingredi with probability 0.0191920243353\n",
      "probabl with probability 0.0190942741689\n",
      "best with probability 0.0187138570491\n",
      "someth with probability 0.0186840118498\n",
      "want with probability 0.017960922743\n",
      "\n",
      "Most likely terms for topic 1:\n",
      "brew with probability 0.0391187770991\n",
      "style with probability 0.0379814285584\n",
      "thi with probability 0.0357220560253\n",
      "lager with probability 0.0275471925447\n",
      "ha with probability 0.0220679113848\n",
      "ani with probability 0.021441369741\n",
      "doe with probability 0.0212927481224\n",
      "gener with probability 0.0209008570249\n",
      "question with probability 0.0185651767617\n",
      "process with probability 0.0179360316264\n",
      "\n",
      "Most likely terms for topic 2:\n",
      "beer with probability 0.054726525156\n",
      "light with probability 0.0275541524642\n",
      "find with probability 0.0257729981648\n",
      "good with probability 0.0232308361683\n",
      "seem with probability 0.0189367762896\n",
      "state with probability 0.0177924401789\n",
      "don with probability 0.0170889498053\n",
      "food with probability 0.0163926515586\n",
      "http with probability 0.0163544091196\n",
      "mean with probability 0.0155795818949\n",
      "\n",
      "Most likely terms for topic 3:\n",
      "beer with probability 0.0797191381591\n",
      "differ with probability 0.0579642721834\n",
      "tast with probability 0.0492442778992\n",
      "bottl with probability 0.0360095023647\n",
      "wine with probability 0.0299139367231\n",
      "time with probability 0.0224592038518\n",
      "type with probability 0.0164866860516\n",
      "might with probability 0.0158286749389\n",
      "guin with probability 0.013647661347\n",
      "pour with probability 0.0134009318614\n",
      "\n",
      "Most likely terms for topic 4:\n",
      "beer with probability 0.128083662938\n",
      "bottl with probability 0.0347971953976\n",
      "will with probability 0.0343922245279\n",
      "wa with probability 0.0326723527567\n",
      "year with probability 0.0234335462137\n",
      "age with probability 0.0202346963019\n",
      "typic with probability 0.0138135558989\n",
      "local with probability 0.0130737189434\n",
      "possibl with probability 0.0126709734335\n",
      "small with probability 0.0123530536416\n",
      "\n",
      "Most likely terms for topic 5:\n",
      "beer with probability 0.0988986335289\n",
      "temperatur with probability 0.0268378959126\n",
      "serv with probability 0.0238175882855\n",
      "wa with probability 0.0229198954399\n",
      "ad with probability 0.0167856885329\n",
      "chang with probability 0.0143688072096\n",
      "similar with probability 0.0139653170678\n",
      "onli with probability 0.0137268007734\n",
      "ani with probability 0.0137266143224\n",
      "cold with probability 0.0128247357658\n",
      "\n",
      "Most likely terms for topic 6:\n",
      "flavor with probability 0.044820277712\n",
      "ale with probability 0.0356159848164\n",
      "hop with probability 0.0321297677432\n",
      "may with probability 0.0310693094386\n",
      "ipa with probability 0.0270887070749\n",
      "much with probability 0.0260253534029\n",
      "tast with probability 0.0183576216976\n",
      "though with probability 0.0175278272144\n",
      "exampl with probability 0.0172270919493\n",
      "effect with probability 0.0164701434181\n",
      "\n",
      "Most likely terms for topic 7:\n",
      "can with probability 0.0515335712705\n",
      "beer with probability 0.0507308152697\n",
      "use with probability 0.0352430860995\n",
      "malt with probability 0.0348541638353\n",
      "bitter with probability 0.0279384444026\n",
      "veri with probability 0.0268345163172\n",
      "will with probability 0.0251147934513\n",
      "carbon with probability 0.0247629550675\n",
      "brewer with probability 0.0200048148268\n",
      "hop with probability 0.0194508924712\n",
      "\n",
      "Most likely terms for topic 8:\n",
      "beer with probability 0.0634667258352\n",
      "breweri with probability 0.0446012563999\n",
      "thi with probability 0.0337649884606\n",
      "store with probability 0.0252330074662\n",
      "look with probability 0.0200588994389\n",
      "make with probability 0.0186494710853\n",
      "sinc with probability 0.0182751837911\n",
      "whi with probability 0.0180994011599\n",
      "call with probability 0.0173708288285\n",
      "onli with probability 0.0173641466\n",
      "\n",
      "Most likely terms for topic 9:\n",
      "alcohol with probability 0.0615908444023\n",
      "glass with probability 0.0441557643066\n",
      "use with probability 0.0434482944074\n",
      "thi with probability 0.0350199635937\n",
      "yeast with probability 0.034897336864\n",
      "ferment with probability 0.0342993044038\n",
      "abv with probability 0.0256495886878\n",
      "water with probability 0.0243008470883\n",
      "sugar with probability 0.0217875968616\n",
      "produc with probability 0.0183284573566\n"
     ]
    }
   ],
   "source": [
    "# HERE BE THE CALL (commented because it takes over an hour to run for the whole set,\n",
    "# so for the first 1000 documents took ~30 minutes)\n",
    "# probs = corpus.lda_gibbs(K = 10, progress_interval = 25, iters = 100)\n",
    "\n",
    "# Print the most likely words for every topic\n",
    "beta = probs['beta']\n",
    "K = beta.shape[0]\n",
    "sorted_beta = np.zeros(beta.shape)\n",
    "\n",
    "for k in range(0,K):\n",
    "    print ''\n",
    "    print 'Most likely terms for topic ' + str(k) + ':'\n",
    "    sorted_beta[k,:] = np.argsort(beta[k,:])[::-1]\n",
    "    for i in range(10):\n",
    "        idx = int(sorted_beta[k,:][i])\n",
    "        print termlist[idx] + ' with probability ' + str(beta.item((k, idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
